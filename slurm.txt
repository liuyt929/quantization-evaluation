#!/bin/bash
#SBATCH --job-name=llama70b
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=1
#SBATCH --gres=gpu:4
#SBATCH --cpus-per-task=12
#SBATCH --time=05:00:00
#SBATCH --output=log-70b-0-%j.out

set -euo pipefail

echo "Job started on $(date)"
echo "Nodes: $SLURM_JOB_NODELIST"
echo "In directory: $(pwd)"

module load brics/nccl brics/aws-ofi-nccl
source $HOME/miniforge3/etc/profile.d/conda.sh
conda activate quantization-eval

# Multi-node setup
MASTER_ADDR=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
MASTER_PORT=29600
NNODES=$SLURM_NNODES
GPUS_PER_NODE=4

export MASTER_ADDR MASTER_PORT
export NCCL_IB_DISABLE=1
export NCCL_SOCKET_IFNAME=eth0 
export NCCL_P2P_LEVEL=SYS
export NCCL_DEBUG=INFO


echo "MASTER_ADDR=$MASTER_ADDR"
echo "MASTER_PORT=$MASTER_PORT"
echo "NNODES=$NNODES"
echo "GPUS_PER_NODE=$GPUS_PER_NODE"

echo "Running: optimize.py with torchrun"

# srun starts one torchrun per node; torchrun handles the multi-GPU within node
# 修正后的 srun 部分
srun --mpi=none bash -c "torchrun \
    --nnodes=\$SLURM_NNODES \
    --nproc_per_node=$GPUS_PER_NODE \
    --node_rank=\$SLURM_PROCID \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    optimize.py \
    --input_model meta-llama/Llama-3.1-70B \
    --model_max_length 2048 \
    --bf16 True \
    --w_bits 4 \
    --a_bits 4 \
    --rotate_ov=True \
    --rotate_post_rope=False \
    --online_gk_hadamard=False \
    --output_dir='./70B-rotate+scale' \
    --smooth_ov=True \
    --a_asym \
    --smooth_up_down=True \
    --smooth_norm_linear=True \
    --train_distribute=False \
    --rank=32 \
    --rotate \
    --enable_low_rank \
    --per_device_train_batch_size=2 \
    --use_klt=False"

echo "Finished optimize.py at $(date)"