import subprocess

# commands=[
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --w_rtn",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank  --w_rtn",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank --w_rtn",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --w_rtn",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym  ",
#     "python ptq_cuda1.py --input_model Qwen/Qwen2.5-3B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth ",
# ]
#asym vs sym
# commands=[
#      "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+a_sym'  --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate",#train sym
#       "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+channel+a_sym'  --train_distribute=True --rank=32  --rotate",#train  sym


# ]
#asym vs sym
commands=[         
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+a_sym'  --w_asym --enable_low_rank --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate",# w_asym svd
#       "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+channel+a_sym' --w_asym --enable_low_rank --train_distribute=True --rank=32  --rotate",# w_asym svd
    #            "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale'  --w_asym --a_asym --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate",#w_asym
    #   "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+channel' --w_asym  --a_asym --train_distribute=True --rank=32  --rotate", #w_asym
    #        "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale'  --w_asym  --a_asym --enable_low_rank --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate",# asym svd
    #   "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+channel' --w_asym  --a_asym --enable_low_rank --train_distribute=True --rank=32  --rotate",# asym svd
                           "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+a_sym'  --w_asym --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate",#w_asym
      "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+channel+a_sym' --w_asym --train_distribute=True --rank=32  --rotate", #w_asym
                 "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+a_sym' --enable_low_rank --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate",#svd sym
      "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+scale+channel+a_sym'   --enable_low_rank --train_distribute=True --rank=32  --rotate",#svd sym
      ]
commands_eval=[
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_32'  --a_asym --a_groupsize=32 --w_groupsize=32 --train_distribute=True --rank=32  --rotate ",
    "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_32'  --a_asym --a_groupsize=32 --w_groupsize=32 --train_distribute=True --rank=32  --rotate ",
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_64'  --a_asym --a_groupsize=64 --w_groupsize=64 --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True --w_clip",
    "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_64'  --a_asym --a_groupsize=64 --w_groupsize=64 --train_distribute=True --rank=32  --rotate ",
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_256'  --a_asym --a_groupsize=256 --w_groupsize=256 --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True --w_clip",
    "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_256'  --a_asym --a_groupsize=256 --w_groupsize=256 --train_distribute=True --rank=32  --rotate ",
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_512'  --a_asym --a_groupsize=512 --w_groupsize=512 --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True --w_clip",
    "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+smooth_512'  --a_asym --a_groupsize=512 --w_groupsize=512 --train_distribute=True --rank=32  --rotate ",
]
# commands_eval=[
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --loss_type=kl_top --post_attn=True --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_qk=True --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --bf16=True --lm_eval=True --output_dir='./output/1B-instruct/rotate+scale+channel' --per_device_train_batch_size=1  --max_steps=100 --w_bits=4 --a_bits=4 --v_bits=16 --k_bits=16 --distribute=True --use_klt"
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/1B-instruct/rotate+scale'  --a_asym --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True",
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/1B-instruct/rotate+scale+channel'  --a_asym  --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True --w_clip",
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/1B-instruct/rotate+scale+channel+a_sym'  --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True",
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=False --smooth_up_down=False --smooth_norm_linear=False --output_dir='./output/1B-instruct/rotate+channel' --a_asym --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True", 
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/1B-instruct/rotate+scale+a_sym'  --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True",
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=False --smooth_up_down=False --smooth_norm_linear=False --output_dir='./output/1B-instruct/rotate' --a_asym  --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32  --rotate --loss_type=kl_top --post_attn=True",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_32'  --a_asym --a_groupsize=32 --w_groupsize=32 --train_distribute=True --rank=32  --rotate",
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_64'  --a_asym --a_groupsize=64 --w_groupsize=64 --train_distribute=True --rank=32  --rotate",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_64'  --a_asym --a_groupsize=64 --w_groupsize=64 --train_distribute=True --rank=32  --rotate",
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_256'  --a_asym --a_groupsize=256 --w_groupsize=256 --train_distribute=True --rank=32  --rotate",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_256'  --a_asym --a_groupsize=256 --w_groupsize=256 --train_distribute=True --rank=32  --rotate",
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_512'  --a_asym --a_groupsize=512 --w_groupsize=512 --train_distribute=True --rank=32  --rotate",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/rotate+smooth_512'  --a_asym --a_groupsize=512 --w_groupsize=512 --train_distribute=True --rank=32  --rotate",
# ]
# command=[
#     "python optimize.py --input_model Qwen/Qwen2.5-3B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate --enable_low_rank --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=False --smooth_norm_linear=True --output_dir='./output/qwen'  --a_asym --a_groupsize=128 --w_groupsize=128 --train_distribute=True --rank=32 --rotate"
#     ]
# commands=[
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --w_rtn",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank  --w_rtn",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank --w_rtn",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --w_rtn",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym  ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank ",
#         "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate --a_asym --smooth --enable_low_rank ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate --a_asym --smooth --w_rtn",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4  --rotate --a_asym --smooth --enable_low_rank  --w_rtn",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank --w_rtn",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --w_rtn",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym  ",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate --a_asym --smooth --enable_low_rank ",
# ] 
# commands_ptq=[
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym  --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --w_rtn --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --enable_low_rank  --w_rtn --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --enable_low_rank --w_rtn --w_clip",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --w_rtn --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=128 --a_groupsize=128 --rotate --a_asym --smooth --w_clip",
#     # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4  --rotate --a_asym --smooth  --w_clip",
# ] 

# command=[
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/rotate+scale' --smooth_ov=True --a_asym --w_asym --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate", #sym sym
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/rotate+scale' --smooth_ov=True --a_asym --w_asym --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate --enable_low_rank", #sym sym

# ]
# command=[
# # "python optimize.py --input_model meta-llama/Llama-3.2-3B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/3B' --a_asym --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate  --enable_low_rank --use_klt=False", #sym sym
# "python optimize.py --input_model Qwen/Qwen2.5-3B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/rotate+scale+qwen' --a_asym --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate --enable_low_rank --use_klt=False", #sym sym
# ]
# command=[
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_asym --a_bits 4  --w_rtn --smooth --w_clip",
#         # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=32 --a_groupsize=32 --smooth --w_rtn --w_clip",

#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --a_asym --smooth --w_clip",
#         # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=32 --a_groupsize=32 --smooth --w_clip",

#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4  --a_asym --enable_low_rank --w_rtn --smooth --w_clip",
#         # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=32 --a_groupsize=32  --enable_low_rank --w_rtn --smooth --w_clip",
        
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4  --a_asym --enable_low_rank --w_clip --smooth",
#         # "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_groupsize=32 --a_groupsize=32  --enable_low_rank  --w_clip --smooth",
   

# ]
# commands=[
#         # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/rotate-group' --a_groupsize=128 --w_groupsize=128 --a_asym  --train_distribute=True --rank=32  --rotate",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --w_groupsize=128 --a_groupsize=128 --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+scale-group' --a_asym  --train_distribute=True --rank=32  --rotate",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --a_groupsize=128 --w_groupsize=128 --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+scale-group' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --a_groupsize=128 --w_groupsize=128 --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+scale-group' --a_asym  --train_distribute=True --rank=32  --rotate --w_rtn",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --a_groupsize=128 --w_groupsize=128 --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/rotate+scale-group' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank --w_rtn",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --a_groupsize=128 --w_groupsize=128 --output_dir='./output/rotate-group' --a_asym  --train_distribute=True --rank=32  --rotate",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --a_groupsize=128 --w_groupsize=128 --output_dir='./output/rotate-group' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank",
#     # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --a_groupsize=128 --w_groupsize=128 --output_dir='./output/rotate-group' --a_asym  --train_distribute=True --rank=32  --rotate --w_rtn",
#     "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 8 --a_bits 8 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/w4a8-origin' --a_asym  --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate --enable_low_rank --w_clip",
# #  "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/instruct-rotate+scale' --a_asym  --train_distribute=True --rank=32  --rotate",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/instruct-rotate+scale' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/instruct-rotate+scale' --a_asym  --train_distribute=True --rank=32  --rotate --w_rtn",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --output_dir='./output/instruct-rotate+scale' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank --w_rtn",
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/instruct-rotate' --a_asym  --train_distribute=True --rank=32  --rotate",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/instruct-rotate' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank",
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/instruct-rotate' --a_asym  --train_distribute=True --rank=32  --rotate --w_rtn"
# # 
# # ,
# #     "python optimize.py --input_model meta-llama/Llama-3.2-1B-Instruct --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/instruct-rotate' --a_asym  --train_distribute=True --rank=32  --rotate --enable_low_rank --w_rtn",
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 8 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/w4a8-nosmooth' --a_asym  --smooth_ov=False --smooth_up_down=False --smooth_norm_linear=False --train_distribute=True --rank=32  --rotate --enable_low_rank --w_clip",

      
# ]


# commands=[
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4  --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/rotate+scale+sr' --smooth_ov=True --a_asym --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate",
# # "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./output/rotate+scale+sr-batchsize1' --smooth_ov=True --a_asym  --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate --enable_low_rank",
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --a_asym  --output_dir='./output/rotate+scale+fp4' --train_distribute=True --rank=32  --rotate --w_rtn --enable_low_rank",
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False  --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True --a_asym --output_dir='./output/rotate+scale+fp4' --train_distribute=True --rank=32  --rotate",
# "python optimize.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False  --smooth_ov=True --smooth_up_down=True --smooth_norm_linear=True  --a_asym --output_dir='./output/rotate+scale+fp4' --train_distribute=True --rank=32  --rotate --enable_low_rank",

# ]
# meta-llama/Llama-3.1-70B
# command=[
#         "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_rtn --a_groupsize=16 --w_groupsize=16",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --a_groupsize=16 --w_groupsize=16",
#     "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --w_rtn  --enable_low_rank --a_groupsize=16 --w_groupsize=16",
# "python ptq.py --input_model meta-llama/Llama-3.2-1B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4 --enable_low_rank --a_groupsize=16 --w_groupsize=16",
# ]

command=[
    "python optimize.py --input_model meta-llama/Llama-3.1-70B --do_train False --do_eval True --model_max_length 2048 --bf16 True --w_bits 4 --a_bits 4  --rotate_ov=True --rotate_post_rope=False --online_qk_hadamard=False --output_dir='./70B-rotate+scale' --smooth_ov=True --a_asym --smooth_up_down=True --smooth_norm_linear=True --train_distribute=True --rank=32  --rotate --enable_low_rank --use_klt=False"
]
for i, cmd in enumerate(command):
    log_file = f"log-70b-{i}.txt"  # 生成不同的日志文件
    with open(log_file, "w") as f:
        print(f"Running: {cmd}")
        process = subprocess.run(cmd, shell=True, stdout=f, stderr=f, text=True)
