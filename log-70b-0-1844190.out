Job started on Wed  7 Jan 01:00:50 UTC 2026
Nodes: nid[011169-011170]
In directory: /projects/u5hv/lionky.u5hv/work_home/quantization-evaluation
MASTER_ADDR=nid011169
MASTER_PORT=29600
NNODES=2
GPUS_PER_NODE=4
Running: optimize.py with torchrun
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
W0107 01:00:59.250859 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] 
W0107 01:00:59.250859 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] *****************************************
W0107 01:00:59.250859 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0107 01:00:59.250859 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] *****************************************
W0107 01:00:59.772001 39178 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] 
W0107 01:00:59.772001 39178 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] *****************************************
W0107 01:00:59.772001 39178 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0107 01:00:59.772001 39178 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py:792] *****************************************
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[rank4]:[W107 01:01:15.607846770 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank0]:[W107 01:01:15.482331665 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[rank7]:[W107 01:01:15.828166083 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[rank5]:[W107 01:01:15.830113005 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank6]:[W107 01:01:15.830509794 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
nid011169:289754:289754 [0] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011169:289754:289754 [0] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011169:289754:289754 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011169:289754:289754 [0] NCCL INFO Bootstrap : Using nmn0:10.100.41.119<0>
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[rank3]:[W107 01:01:15.680445979 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404
[rank1]:[W107 01:01:15.684547106 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
[rank2]:[W107 01:01:15.684577857 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
nid011169:289754:289754 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011169:289754:289754 [0] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011169:289754:289754 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011169:289754:289754 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011169:289754:289754 [0] NCCL INFO cudaDriverVersion 12070
NCCL version 2.21.5+cuda12.6
nid011169:289754:289754 [0] NCCL INFO Comm config Blocking set to 1
nid011169:289755:289755 [1] NCCL INFO cudaDriverVersion 12070
nid011169:289757:289757 [3] NCCL INFO cudaDriverVersion 12070
nid011169:289756:289756 [2] NCCL INFO cudaDriverVersion 12070
nid011169:289755:289755 [1] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011169:289757:289757 [3] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011169:289756:289756 [2] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011169:289755:289755 [1] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011169:289755:289755 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011169:289757:289757 [3] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011169:289757:289757 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011169:289756:289756 [2] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011169:289756:289756 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011170:39197:39197 [2] NCCL INFO cudaDriverVersion 12070
nid011169:289755:289755 [1] NCCL INFO Bootstrap : Using nmn0:10.100.41.119<0>
nid011169:289756:289756 [2] NCCL INFO Bootstrap : Using nmn0:10.100.41.119<0>
nid011169:289757:289757 [3] NCCL INFO Bootstrap : Using nmn0:10.100.41.119<0>
nid011170:39195:39195 [0] NCCL INFO cudaDriverVersion 12070
nid011170:39196:39196 [1] NCCL INFO cudaDriverVersion 12070
nid011170:39198:39198 [3] NCCL INFO cudaDriverVersion 12070
nid011170:39196:39196 [1] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011170:39197:39197 [2] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011170:39195:39195 [0] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011170:39198:39198 [3] NCCL INFO NCCL_GDRCOPY_ENABLE set by environment to 1.
nid011170:39197:39197 [2] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011170:39195:39195 [0] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011170:39197:39197 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011170:39195:39195 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011170:39196:39196 [1] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011170:39196:39196 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011170:39198:39198 [3] NCCL INFO GDRCOPY enabled library 2.4 driver 2.4
nid011170:39198:39198 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
nid011170:39195:39195 [0] NCCL INFO Bootstrap : Using nmn0:10.100.42.165<0>
nid011170:39196:39196 [1] NCCL INFO Bootstrap : Using nmn0:10.100.42.165<0>
nid011170:39197:39197 [2] NCCL INFO Bootstrap : Using nmn0:10.100.42.165<0>
nid011170:39198:39198 [3] NCCL INFO Bootstrap : Using nmn0:10.100.42.165<0>
nid011169:289756:289756 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011169:289756:289756 [2] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011169:289756:289756 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011169:289756:289756 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011169:289755:289755 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011169:289755:289755 [1] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011169:289755:289755 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011169:289755:289755 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011169:289757:289757 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011169:289757:289757 [3] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011169:289757:289757 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011169:289757:289757 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011169:289756:289756 [2] NCCL INFO Comm config Blocking set to 1
nid011169:289755:289755 [1] NCCL INFO Comm config Blocking set to 1
nid011169:289757:289757 [3] NCCL INFO Comm config Blocking set to 1
nid011170:39195:39195 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011170:39196:39196 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011170:39195:39195 [0] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011170:39195:39195 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011170:39195:39195 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011170:39198:39198 [3] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011170:39196:39196 [1] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011170:39198:39198 [3] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011170:39196:39196 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011170:39198:39198 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011170:39196:39196 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011170:39198:39198 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011170:39198:39198 [3] NCCL INFO Comm config Blocking set to 1
nid011170:39195:39195 [0] NCCL INFO Comm config Blocking set to 1
nid011170:39196:39196 [1] NCCL INFO Comm config Blocking set to 1
nid011170:39197:39197 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v8 symbol.
nid011170:39197:39197 [2] NCCL INFO NET/Plugin: Loaded net plugin AWS Libfabric (v7)
nid011170:39197:39197 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
nid011170:39197:39197 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.
nid011170:39197:39197 [2] NCCL INFO Comm config Blocking set to 1
nid011169:289754:289820 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289754:289820 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289754:289820 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011169:289756:289821 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289756:289821 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289756:289821 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011169:289755:289822 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289755:289822 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289755:289822 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011169:289757:289823 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289757:289823 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011169:289757:289823 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011169:289755:289822 [1] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011169:289757:289823 [3] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011169:289756:289821 [2] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011169:289754:289820 [0] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011169:289756:289821 [2] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011169:289756:289821 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011169:289754:289820 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011169:289754:289820 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011169:289755:289822 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011169:289755:289822 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011169:289757:289823 [3] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011169:289757:289823 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011169:289754:289820 [0] NCCL INFO Using non-device net plugin version 0
nid011169:289754:289820 [0] NCCL INFO Using network AWS Libfabric
nid011169:289755:289822 [1] NCCL INFO Using non-device net plugin version 0
nid011169:289755:289822 [1] NCCL INFO Using network AWS Libfabric
nid011169:289756:289821 [2] NCCL INFO Using non-device net plugin version 0
nid011169:289756:289821 [2] NCCL INFO Using network AWS Libfabric
nid011169:289757:289823 [3] NCCL INFO Using non-device net plugin version 0
nid011169:289757:289823 [3] NCCL INFO Using network AWS Libfabric
nid011170:39195:39257 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39195:39257 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39195:39257 [0] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011170:39198:39255 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39198:39255 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39198:39255 [3] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011170:39197:39258 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39196:39256 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39197:39258 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39196:39256 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.8.1-aws
nid011170:39196:39256 [1] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011170:39197:39258 [2] NCCL INFO NET/OFI Using Libfabric version 1.22
nid011170:39195:39257 [0] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011170:39197:39258 [2] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011170:39196:39256 [1] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011170:39198:39255 [3] NCCL INFO NET/OFI Using CUDA driver version 12070
nid011170:39196:39256 [1] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011170:39196:39256 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011170:39195:39257 [0] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011170:39195:39257 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011170:39198:39255 [3] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011170:39198:39255 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011170:39197:39258 [2] NCCL INFO NET/OFI Selected Provider is cxi (found 4 nics)
nid011170:39197:39258 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
nid011170:39196:39256 [1] NCCL INFO Using non-device net plugin version 0
nid011170:39195:39257 [0] NCCL INFO Using non-device net plugin version 0
nid011170:39196:39256 [1] NCCL INFO Using network AWS Libfabric
nid011170:39195:39257 [0] NCCL INFO Using network AWS Libfabric
nid011170:39197:39258 [2] NCCL INFO Using non-device net plugin version 0
nid011170:39197:39258 [2] NCCL INFO Using network AWS Libfabric
nid011170:39198:39255 [3] NCCL INFO Using non-device net plugin version 0
nid011170:39198:39255 [3] NCCL INFO Using network AWS Libfabric
nid011169:289757:289823 [3] NCCL INFO DMA-BUF is available on GPU device 3
nid011169:289755:289822 [1] NCCL INFO DMA-BUF is available on GPU device 1
nid011169:289756:289821 [2] NCCL INFO DMA-BUF is available on GPU device 2
nid011169:289754:289820 [0] NCCL INFO DMA-BUF is available on GPU device 0
nid011170:39197:39258 [2] NCCL INFO DMA-BUF is available on GPU device 2
nid011170:39198:39255 [3] NCCL INFO DMA-BUF is available on GPU device 3
nid011170:39196:39256 [1] NCCL INFO DMA-BUF is available on GPU device 1
nid011170:39195:39257 [0] NCCL INFO DMA-BUF is available on GPU device 0
nid011169:289757:289823 [3] NCCL INFO ncclCommInitRank comm 0xaaad693e1c60 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x3c93adf3209e108c - Init START
nid011169:289756:289821 [2] NCCL INFO ncclCommInitRank comm 0xaaacb7991bc0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x3c93adf3209e108c - Init START
nid011169:289754:289820 [0] NCCL INFO ncclCommInitRank comm 0xaaaed3672f40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 901000 commId 0x3c93adf3209e108c - Init START
nid011169:289755:289822 [1] NCCL INFO ncclCommInitRank comm 0xaaad7c1e20a0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x3c93adf3209e108c - Init START
nid011170:39197:39258 [2] NCCL INFO ncclCommInitRank comm 0xaaae160b37f0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x3c93adf3209e108c - Init START
nid011170:39198:39255 [3] NCCL INFO ncclCommInitRank comm 0xaaacc4801a80 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x3c93adf3209e108c - Init START
nid011170:39195:39257 [0] NCCL INFO ncclCommInitRank comm 0xaaab58861270 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 901000 commId 0x3c93adf3209e108c - Init START
nid011170:39196:39256 [1] NCCL INFO ncclCommInitRank comm 0xaaae391232b0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x3c93adf3209e108c - Init START
nid011169:289755:289822 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011169:289755:289822 [1] NCCL INFO NVLS multicast support is not available on dev 1
nid011169:289755:289822 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011169:289754:289820 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011169:289754:289820 [0] NCCL INFO Setting affinity for GPU 0 to 3f
nid011169:289754:289820 [0] NCCL INFO NVLS multicast support is not available on dev 0
nid011169:289754:289820 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011169:289757:289823 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011169:289757:289823 [3] NCCL INFO NVLS multicast support is not available on dev 3
nid011169:289756:289821 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011169:289757:289823 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011169:289756:289821 [2] NCCL INFO NVLS multicast support is not available on dev 2
nid011169:289756:289821 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011170:39198:39255 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011170:39198:39255 [3] NCCL INFO NVLS multicast support is not available on dev 3
nid011170:39198:39255 [3] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011170:39196:39256 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011170:39196:39256 [1] NCCL INFO NVLS multicast support is not available on dev 1
nid011170:39196:39256 [1] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011170:39197:39258 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011170:39195:39257 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to SYS
nid011170:39197:39258 [2] NCCL INFO NVLS multicast support is not available on dev 2
nid011170:39197:39258 [2] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011170:39195:39257 [0] NCCL INFO Setting affinity for GPU 0 to 3f
nid011170:39195:39257 [0] NCCL INFO NVLS multicast support is not available on dev 0
nid011170:39195:39257 [0] NCCL INFO NCCL_CROSS_NIC set by environment to 0.
nid011169:289757:289823 [3] NCCL INFO comm 0xaaad693e1c60 rank 3 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
nid011169:289757:289823 [3] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011169:289757:289823 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] 0/-1/-1->3->2 [2] 0/-1/-1->3->2 [3] 0/7/-1->3->-1 [4] -1/-1/-1->3->2 [5] 0/-1/-1->3->2 [6] 0/-1/-1->3->2 [7] 0/7/-1->3->-1 [8] -1/-1/-1->3->2 [9] 0/-1/-1->3->2 [10] 0/-1/-1->3->2 [11] 0/7/-1->3->-1 [12] -1/-1/-1->3->2 [13] 0/-1/-1->3->2 [14] 0/-1/-1->3->2 [15] 0/-1/-1->3->7 [16] -1/-1/-1->3->2 [17] 0/-1/-1->3->2 [18] 0/-1/-1->3->2 [19] 0/-1/-1->3->7 [20] -1/-1/-1->3->2 [21] 0/-1/-1->3->2 [22] 0/-1/-1->3->2 [23] 0/-1/-1->3->7
nid011169:289757:289823 [3] NCCL INFO P2P Chunksize set to 131072
nid011169:289754:289820 [0] NCCL INFO comm 0xaaaed3672f40 rank 0 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
nid011169:289756:289821 [2] NCCL INFO comm 0xaaacb7991bc0 rank 2 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
nid011169:289755:289822 [1] NCCL INFO comm 0xaaad7c1e20a0 rank 1 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
nid011169:289754:289820 [0] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011169:289756:289821 [2] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011169:289755:289822 [1] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011169:289754:289820 [0] NCCL INFO Channel 00/24 :    0   3   2   1   4   7   6   5
nid011169:289754:289820 [0] NCCL INFO Channel 01/24 :    0   3   2   5   4   7   6   1
nid011169:289754:289820 [0] NCCL INFO Channel 02/24 :    0   3   6   5   4   7   2   1
nid011169:289754:289820 [0] NCCL INFO Channel 03/24 :    0   7   6   5   4   3   2   1
nid011169:289754:289820 [0] NCCL INFO Channel 04/24 :    0   3   1   2   4   7   5   6
nid011169:289754:289820 [0] NCCL INFO Channel 05/24 :    0   2   3   5   4   6   7   1
nid011169:289756:289821 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/6/-1->2->-1 [3] -1/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/6/-1->2->-1 [7] -1/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/6/-1->2->-1 [11] -1/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->6 [15] -1/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->6 [19] -1/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->6 [23] -1/-1/-1->2->1
nid011169:289754:289820 [0] NCCL INFO Channel 06/24 :    0   6   5   7   4   2   1   3
nid011169:289755:289822 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/5/-1->1->-1 [2] -1/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/5/-1->1->-1 [6] -1/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/5/-1->1->-1 [10] -1/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->5 [14] -1/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->5 [18] -1/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->5 [22] -1/-1/-1->1->0 [23] 2/-1/-1->1->0
nid011169:289754:289820 [0] NCCL INFO Channel 07/24 :    0   1   7   6   4   5   3   2
nid011169:289756:289821 [2] NCCL INFO P2P Chunksize set to 131072
nid011169:289754:289820 [0] NCCL INFO Channel 08/24 :    0   2   1   3   4   6   5   7
nid011169:289755:289822 [1] NCCL INFO P2P Chunksize set to 131072
nid011169:289754:289820 [0] NCCL INFO Channel 09/24 :    0   5   7   6   4   1   3   2
nid011169:289754:289820 [0] NCCL INFO Channel 10/24 :    0   3   1   6   4   7   5   2
nid011169:289754:289820 [0] NCCL INFO Channel 11/24 :    0   2   7   5   4   6   3   1
nid011169:289754:289820 [0] NCCL INFO Channel 12/24 :    0   3   2   1   4   7   6   5
nid011170:39198:39255 [3] NCCL INFO comm 0xaaacc4801a80 rank 7 nRanks 8 nNodes 2 localRanks 4 localRank 3 MNNVL 0
nid011170:39196:39256 [1] NCCL INFO comm 0xaaae391232b0 rank 5 nRanks 8 nNodes 2 localRanks 4 localRank 1 MNNVL 0
nid011170:39197:39258 [2] NCCL INFO comm 0xaaae160b37f0 rank 6 nRanks 8 nNodes 2 localRanks 4 localRank 2 MNNVL 0
nid011170:39195:39257 [0] NCCL INFO comm 0xaaab58861270 rank 4 nRanks 8 nNodes 2 localRanks 4 localRank 0 MNNVL 0
nid011169:289754:289820 [0] NCCL INFO Channel 13/24 :    0   3   2   5   4   7   6   1
nid011169:289754:289820 [0] NCCL INFO Channel 14/24 :    0   3   6   5   4   7   2   1
nid011169:289754:289820 [0] NCCL INFO Channel 15/24 :    0   7   6   5   4   3   2   1
nid011169:289754:289820 [0] NCCL INFO Channel 16/24 :    0   3   1   2   4   7   5   6
nid011169:289754:289820 [0] NCCL INFO Channel 17/24 :    0   2   3   5   4   6   7   1
nid011169:289754:289820 [0] NCCL INFO Channel 18/24 :    0   6   5   7   4   2   1   3
nid011169:289754:289820 [0] NCCL INFO Channel 19/24 :    0   1   7   6   4   5   3   2
nid011169:289754:289820 [0] NCCL INFO Channel 20/24 :    0   2   1   3   4   6   5   7
nid011169:289754:289820 [0] NCCL INFO Channel 21/24 :    0   5   7   6   4   1   3   2
nid011169:289754:289820 [0] NCCL INFO Channel 22/24 :    0   3   1   6   4   7   5   2
nid011169:289754:289820 [0] NCCL INFO Channel 23/24 :    0   2   7   5   4   6   3   1
nid011170:39198:39255 [3] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011170:39196:39256 [1] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011170:39197:39258 [2] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011170:39195:39257 [0] NCCL INFO NCCL_MIN_NCHANNELS set by environment to 4.
nid011169:289754:289820 [0] NCCL INFO Trees [0] 1/4/-1->0->-1 [1] -1/-1/-1->0->3 [2] 1/-1/-1->0->3 [3] 1/-1/-1->0->3 [4] 1/4/-1->0->-1 [5] -1/-1/-1->0->3 [6] 1/-1/-1->0->3 [7] 1/-1/-1->0->3 [8] 1/4/-1->0->-1 [9] -1/-1/-1->0->3 [10] 1/-1/-1->0->3 [11] 1/-1/-1->0->3 [12] 1/-1/-1->0->4 [13] -1/-1/-1->0->3 [14] 1/-1/-1->0->3 [15] 1/-1/-1->0->3 [16] 1/-1/-1->0->4 [17] -1/-1/-1->0->3 [18] 1/-1/-1->0->3 [19] 1/-1/-1->0->3 [20] 1/-1/-1->0->4 [21] -1/-1/-1->0->3 [22] 1/-1/-1->0->3 [23] 1/-1/-1->0->3
nid011169:289754:289820 [0] NCCL INFO P2P Chunksize set to 131072
nid011170:39196:39256 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->1 [2] -1/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->1 [6] -1/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->1 [10] -1/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/1/-1->5->-1 [14] -1/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/1/-1->5->-1 [18] -1/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/1/-1->5->-1 [22] -1/-1/-1->5->4 [23] 6/-1/-1->5->4
nid011170:39198:39255 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 4/-1/-1->7->6 [2] 4/-1/-1->7->6 [3] 4/-1/-1->7->3 [4] -1/-1/-1->7->6 [5] 4/-1/-1->7->6 [6] 4/-1/-1->7->6 [7] 4/-1/-1->7->3 [8] -1/-1/-1->7->6 [9] 4/-1/-1->7->6 [10] 4/-1/-1->7->6 [11] 4/-1/-1->7->3 [12] -1/-1/-1->7->6 [13] 4/-1/-1->7->6 [14] 4/-1/-1->7->6 [15] 4/3/-1->7->-1 [16] -1/-1/-1->7->6 [17] 4/-1/-1->7->6 [18] 4/-1/-1->7->6 [19] 4/3/-1->7->-1 [20] -1/-1/-1->7->6 [21] 4/-1/-1->7->6 [22] 4/-1/-1->7->6 [23] 4/3/-1->7->-1
nid011170:39197:39258 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->2 [3] -1/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->2 [7] -1/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->2 [11] -1/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/2/-1->6->-1 [15] -1/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/2/-1->6->-1 [19] -1/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/2/-1->6->-1 [23] -1/-1/-1->6->5
nid011170:39196:39256 [1] NCCL INFO P2P Chunksize set to 131072
nid011170:39195:39257 [0] NCCL INFO Trees [0] 5/-1/-1->4->0 [1] -1/-1/-1->4->7 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7 [4] 5/-1/-1->4->0 [5] -1/-1/-1->4->7 [6] 5/-1/-1->4->7 [7] 5/-1/-1->4->7 [8] 5/-1/-1->4->0 [9] -1/-1/-1->4->7 [10] 5/-1/-1->4->7 [11] 5/-1/-1->4->7 [12] 5/0/-1->4->-1 [13] -1/-1/-1->4->7 [14] 5/-1/-1->4->7 [15] 5/-1/-1->4->7 [16] 5/0/-1->4->-1 [17] -1/-1/-1->4->7 [18] 5/-1/-1->4->7 [19] 5/-1/-1->4->7 [20] 5/0/-1->4->-1 [21] -1/-1/-1->4->7 [22] 5/-1/-1->4->7 [23] 5/-1/-1->4->7
nid011170:39198:39255 [3] NCCL INFO P2P Chunksize set to 131072
nid011170:39197:39258 [2] NCCL INFO P2P Chunksize set to 131072
nid011170:39195:39257 [0] NCCL INFO P2P Chunksize set to 131072
nid011169:289754:289820 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011169:289754:289820 [0] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011169:289754:289820 [0] NCCL INFO Channel 08/0 : 7[3] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 20/0 : 7[3] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011170:39195:39257 [0] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011169:289757:289823 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011169:289757:289823 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[0] [send] via NET/AWS Libfabric/0
nid011170:39197:39258 [2] NCCL INFO Channel 05/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[0] [send] via NET/AWS Libfabric/0
nid011170:39196:39256 [1] NCCL INFO Channel 04/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011170:39198:39255 [3] NCCL INFO Channel 08/0 : 7[3] -> 0[0] [send] via NET/AWS Libfabric/0
nid011170:39198:39255 [3] NCCL INFO Channel 20/0 : 7[3] -> 0[0] [send] via NET/AWS Libfabric/0
nid011169:289756:289821 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011169:289755:289822 [1] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011170:39197:39258 [2] NCCL INFO Channel 17/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 05/0 : 7[3] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 17/0 : 7[3] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 06/0 : 1[1] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 08/0 : 3[3] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 05/0 : 7[3] -> 1[1] [send] via NET/AWS Libfabric/1
nid011170:39198:39255 [3] NCCL INFO Channel 17/0 : 7[3] -> 1[1] [send] via NET/AWS Libfabric/1
nid011170:39196:39256 [1] NCCL INFO Channel 16/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011170:39197:39258 [2] NCCL INFO Channel 04/0 : 6[2] -> 0[0] [send] via NET/AWS Libfabric/0
nid011169:289755:289822 [1] NCCL INFO Channel 08/0 : 1[1] -> 3[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 09/0 : 1[1] -> 3[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 18/0 : 1[1] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 20/0 : 3[3] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 07/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 16/0 : 6[2] -> 0[0] [send] via NET/AWS Libfabric/0
nid011170:39195:39257 [0] NCCL INFO Channel 19/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011170:39196:39256 [1] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011169:289754:289820 [0] NCCL INFO Channel 04/0 : 6[2] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 05/0 : 3[3] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 05/0 : 3[3] -> 5[1] [send] via NET/AWS Libfabric/1
nid011169:289757:289823 [3] NCCL INFO Channel 17/0 : 3[3] -> 5[1] [send] via NET/AWS Libfabric/1
nid011169:289754:289820 [0] NCCL INFO Channel 16/0 : 6[2] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 05/0 : 0[0] -> 2[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 08/0 : 0[0] -> 2[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 11/0 : 0[0] -> 2[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 17/0 : 0[0] -> 2[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 20/0 : 0[0] -> 2[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 23/0 : 0[0] -> 2[2] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 04/0 : 2[2] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 16/0 : 2[2] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 05/0 : 4[0] -> 6[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 20/0 : 1[1] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 08/0 : 4[0] -> 6[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 21/0 : 1[1] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 11/0 : 4[0] -> 6[2] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 17/0 : 4[0] -> 6[2] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 20/0 : 4[0] -> 6[2] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 23/0 : 4[0] -> 6[2] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO NCCL_NET_GDR_LEVEL set by environment to PHB
nid011170:39196:39256 [1] NCCL INFO Channel 17/0 : 3[3] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 06/0 : 5[1] -> 7[3] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 08/0 : 5[1] -> 7[3] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 09/0 : 5[1] -> 7[3] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 18/0 : 5[1] -> 7[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 01/0 : 6[2] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 13/0 : 6[2] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 20/0 : 5[1] -> 7[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 00/0 : 1[1] -> 4[0] [send] via NET/AWS Libfabric/0
nid011169:289755:289822 [1] NCCL INFO Channel 12/0 : 1[1] -> 4[0] [send] via NET/AWS Libfabric/0
nid011170:39196:39256 [1] NCCL INFO Channel 21/0 : 5[1] -> 7[3] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 01/0 : 2[2] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 13/0 : 2[2] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 00/0 : 5[1] -> 0[0] [send] via NET/AWS Libfabric/0
nid011170:39196:39256 [1] NCCL INFO Channel 12/0 : 5[1] -> 0[0] [send] via NET/AWS Libfabric/0
nid011170:39197:39258 [2] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011170:39197:39258 [2] NCCL INFO Channel 02/0 : 3[3] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 14/0 : 3[3] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 01/0 : 6[2] -> 1[1] [send] via NET/AWS Libfabric/1
nid011170:39197:39258 [2] NCCL INFO Channel 13/0 : 6[2] -> 1[1] [send] via NET/AWS Libfabric/1
nid011169:289756:289821 [2] NCCL INFO Channel 04/0 : 2[2] -> 4[0] [send] via NET/AWS Libfabric/0
nid011169:289756:289821 [2] NCCL INFO Channel 16/0 : 2[2] -> 4[0] [send] via NET/AWS Libfabric/0
nid011169:289754:289820 [0] NCCL INFO Channel 00/0 : 5[1] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 12/0 : 5[1] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289756:289821 [2] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011169:289754:289820 [0] NCCL INFO Channel 00/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 01/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 02/0 : 7[3] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 14/0 : 7[3] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 01/0 : 2[2] -> 5[1] [send] via NET/AWS Libfabric/1
nid011169:289756:289821 [2] NCCL INFO Channel 13/0 : 2[2] -> 5[1] [send] via NET/AWS Libfabric/1
nid011169:289757:289823 [3] NCCL INFO Channel 02/0 : 3[3] -> 6[2] [send] via NET/AWS Libfabric/2
nid011169:289757:289823 [3] NCCL INFO Channel 14/0 : 3[3] -> 6[2] [send] via NET/AWS Libfabric/2
nid011170:39197:39258 [2] NCCL INFO Channel 10/0 : 1[1] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 22/0 : 1[1] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 11/0 : 6[2] -> 3[3] [send] via NET/AWS Libfabric/3
nid011170:39197:39258 [2] NCCL INFO Channel 23/0 : 6[2] -> 3[3] [send] via NET/AWS Libfabric/3
nid011170:39195:39257 [0] NCCL INFO Channel 00/0 : 1[1] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 12/0 : 1[1] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 00/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 01/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 02/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 04/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 10/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 12/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 13/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 02/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 04/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 10/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 12/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 13/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 14/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 16/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 22/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011169:289757:289823 [3] NCCL INFO Channel 11/0 : 6[2] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 23/0 : 6[2] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 14/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 16/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 22/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 09/0 : 4[0] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 21/0 : 4[0] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 02/0 : 7[3] -> 2[2] [send] via NET/AWS Libfabric/2
nid011170:39198:39255 [3] NCCL INFO Channel 14/0 : 7[3] -> 2[2] [send] via NET/AWS Libfabric/2
nid011169:289755:289822 [1] NCCL INFO Channel 10/0 : 1[1] -> 6[2] [send] via NET/AWS Libfabric/2
nid011169:289755:289822 [1] NCCL INFO Channel 22/0 : 1[1] -> 6[2] [send] via NET/AWS Libfabric/2
nid011170:39198:39255 [3] NCCL INFO NCCL_NET_FORCE_FLUSH set by environment to 1.
nid011170:39197:39258 [2] NCCL INFO Channel 06/0 : 0[0] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 18/0 : 0[0] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 07/0 : 6[2] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 09/0 : 6[2] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 11/0 : 2[2] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 23/0 : 2[2] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 06/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 18/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 10/0 : 6[2] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 19/0 : 6[2] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 21/0 : 6[2] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 22/0 : 6[2] -> 4[0] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 09/0 : 0[0] -> 5[1] [send] via NET/AWS Libfabric/1
nid011169:289754:289820 [0] NCCL INFO Channel 21/0 : 0[0] -> 5[1] [send] via NET/AWS Libfabric/1
nid011169:289757:289823 [3] NCCL INFO Channel 07/0 : 5[1] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 19/0 : 5[1] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 04/0 : 3[3] -> 1[1] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 10/0 : 3[3] -> 1[1] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 11/0 : 3[3] -> 1[1] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 16/0 : 3[3] -> 1[1] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 22/0 : 3[3] -> 1[1] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 09/0 : 0[0] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 21/0 : 0[0] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 09/0 : 4[0] -> 1[1] [send] via NET/AWS Libfabric/1
nid011170:39195:39257 [0] NCCL INFO Channel 21/0 : 4[0] -> 1[1] [send] via NET/AWS Libfabric/1
nid011169:289755:289822 [1] NCCL INFO Channel 07/0 : 1[1] -> 7[3] [send] via NET/AWS Libfabric/3
nid011169:289755:289822 [1] NCCL INFO Channel 19/0 : 1[1] -> 7[3] [send] via NET/AWS Libfabric/3
nid011170:39196:39256 [1] NCCL INFO Channel 10/0 : 5[1] -> 2[2] [send] via NET/AWS Libfabric/2
nid011170:39196:39256 [1] NCCL INFO Channel 22/0 : 5[1] -> 2[2] [send] via NET/AWS Libfabric/2
nid011170:39195:39257 [0] NCCL INFO Channel 06/0 : 4[0] -> 2[2] [send] via NET/AWS Libfabric/2
nid011170:39195:39257 [0] NCCL INFO Channel 18/0 : 4[0] -> 2[2] [send] via NET/AWS Libfabric/2
nid011169:289756:289821 [2] NCCL INFO Channel 10/0 : 5[1] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 22/0 : 5[1] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 06/0 : 0[0] -> 6[2] [send] via NET/AWS Libfabric/2
nid011169:289756:289821 [2] NCCL INFO Channel 11/0 : 2[2] -> 7[3] [send] via NET/AWS Libfabric/3
nid011169:289754:289820 [0] NCCL INFO Channel 18/0 : 0[0] -> 6[2] [send] via NET/AWS Libfabric/2
nid011169:289757:289823 [3] NCCL INFO Channel 23/0 : 3[3] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 23/0 : 2[2] -> 7[3] [send] via NET/AWS Libfabric/3
nid011169:289756:289821 [2] NCCL INFO Channel 06/0 : 4[0] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 18/0 : 4[0] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 07/0 : 2[2] -> 0[0] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 09/0 : 2[2] -> 0[0] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 10/0 : 2[2] -> 0[0] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 19/0 : 2[2] -> 0[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 07/0 : 5[1] -> 3[3] [send] via NET/AWS Libfabric/3
nid011170:39196:39256 [1] NCCL INFO Channel 19/0 : 5[1] -> 3[3] [send] via NET/AWS Libfabric/3
nid011169:289757:289823 [3] NCCL INFO Channel 03/0 : 4[0] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 15/0 : 4[0] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 07/0 : 1[1] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 19/0 : 1[1] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 04/0 : 7[3] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 10/0 : 7[3] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 11/0 : 7[3] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 16/0 : 7[3] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 22/0 : 7[3] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 23/0 : 7[3] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 03/0 : 0[0] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 15/0 : 0[0] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 00/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 01/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 01/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 02/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 03/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 05/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 11/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 21/0 : 2[2] -> 0[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 03/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 07/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 22/0 : 2[2] -> 0[0] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 03/0 : 4[0] -> 3[3] [send] via NET/AWS Libfabric/3
nid011170:39195:39257 [0] NCCL INFO Channel 15/0 : 4[0] -> 3[3] [send] via NET/AWS Libfabric/3
nid011170:39198:39255 [3] NCCL INFO Channel 09/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 13/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 00/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 02/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 03/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 03/0 : 0[0] -> 7[3] [send] via NET/AWS Libfabric/3
nid011169:289754:289820 [0] NCCL INFO Channel 15/0 : 0[0] -> 7[3] [send] via NET/AWS Libfabric/3
nid011170:39197:39258 [2] NCCL INFO Channel 06/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 08/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 12/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 13/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 15/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 19/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 14/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 15/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 17/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 12/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 14/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 15/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 18/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 20/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 21/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 23/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Connected all rings
nid011169:289756:289821 [2] NCCL INFO Connected all rings
nid011169:289755:289822 [1] NCCL INFO Connected all rings
nid011169:289754:289820 [0] NCCL INFO Connected all rings
nid011169:289754:289820 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Connected all rings
nid011170:39196:39256 [1] NCCL INFO Connected all rings
nid011169:289757:289823 [3] NCCL INFO Connected all rings
nid011170:39195:39257 [0] NCCL INFO Connected all rings
nid011170:39195:39257 [0] NCCL INFO Channel 00/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 02/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 03/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 04/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 06/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 03/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 05/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 06/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 08/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 01/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 05/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 09/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 13/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 07/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 09/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 10/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 11/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 12/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 14/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 15/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 16/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 18/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 20/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 22/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 23/0 : 4[0] -> 5[1] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 11/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 15/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 17/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 18/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 17/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 21/0 : 5[1] -> 1[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 01/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 05/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 09/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 13/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 17/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 21/0 : 1[1] -> 5[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 02/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 10/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 14/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 00/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 01/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 02/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 04/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 06/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 08/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 19/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 09/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 18/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 10/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 12/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 22/0 : 6[2] -> 2[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 02/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 21/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 23/0 : 0[0] -> 3[3] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 00/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011169:289754:289820 [0] NCCL INFO Channel 00/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 07/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 13/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 14/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 16/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 18/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 20/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 10/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 08/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 12/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 16/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 20/0 : 4[0] -> 0[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 00/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 08/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 12/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 16/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 11/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289754:289820 [0] NCCL INFO Channel 20/0 : 0[0] -> 4[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 15/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 01/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 03/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 21/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 22/0 : 6[2] -> 7[3] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 19/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 23/0 : 7[3] -> 3[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 07/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 11/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 15/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 19/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289757:289823 [3] NCCL INFO Channel 23/0 : 3[3] -> 7[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 14/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 18/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011169:289756:289821 [2] NCCL INFO Channel 22/0 : 2[2] -> 6[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 05/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 07/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 08/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 09/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 11/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 12/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 13/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 15/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 17/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 19/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 20/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 21/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 23/0 : 5[1] -> 6[2] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 01/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 02/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 06/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 10/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 14/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 18/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 22/0 : 2[2] -> 6[2] [receive] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 02/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 06/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 10/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 14/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 18/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 22/0 : 6[2] -> 2[2] [send] via NET/AWS Libfabric/2/GDRDMA
nid011170:39197:39258 [2] NCCL INFO Channel 01/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 04/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 05/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 07/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 09/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 11/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 13/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 16/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 17/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 05/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 06/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 07/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011169:289756:289821 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 19/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 21/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39197:39258 [2] NCCL INFO Channel 23/0 : 6[2] -> 5[1] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 05/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 09/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 13/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 17/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 21/0 : 1[1] -> 5[1] [receive] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 01/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 05/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 09/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 13/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 17/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011170:39196:39256 [1] NCCL INFO Channel 21/0 : 5[1] -> 1[1] [send] via NET/AWS Libfabric/1/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 09/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 11/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 15/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 17/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 18/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 19/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 21/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39195:39257 [0] NCCL INFO Channel 23/0 : 4[0] -> 7[3] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 03/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 00/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 04/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 07/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 08/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 12/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 16/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 20/0 : 0[0] -> 4[0] [receive] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 00/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 04/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 08/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 12/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 16/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011170:39195:39257 [0] NCCL INFO Channel 20/0 : 4[0] -> 0[0] [send] via NET/AWS Libfabric/0/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 11/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 15/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 19/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 23/0 : 3[3] -> 7[3] [receive] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 03/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 07/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 11/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 15/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 19/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011169:289755:289822 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 00/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 23/0 : 7[3] -> 3[3] [send] via NET/AWS Libfabric/3/GDRDMA
nid011170:39198:39255 [3] NCCL INFO Channel 01/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 02/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 03/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 05/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 07/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 09/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 10/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 11/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 13/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 04/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 06/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 07/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 08/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 10/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 14/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 15/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 12/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 16/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 18/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 19/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 20/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011170:39196:39256 [1] NCCL INFO Channel 22/0 : 5[1] -> 4[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011169:289755:289822 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 17/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 19/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 21/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 22/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 23/0 : 7[3] -> 4[0] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 02/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 04/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 05/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 06/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 08/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 10/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 14/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 16/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 17/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 18/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 20/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011170:39198:39255 [3] NCCL INFO Channel 22/0 : 7[3] -> 6[2] via P2P/CUMEM
nid011169:289757:289823 [3] NCCL INFO Connected all trees
nid011169:289757:289823 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011169:289757:289823 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011170:39198:39255 [3] NCCL INFO Connected all trees
nid011170:39198:39255 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011170:39198:39255 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011169:289754:289820 [0] NCCL INFO Connected all trees
nid011169:289754:289820 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011169:289754:289820 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011169:289756:289821 [2] NCCL INFO Connected all trees
nid011169:289755:289822 [1] NCCL INFO Connected all trees
nid011169:289756:289821 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011169:289756:289821 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011169:289755:289822 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011169:289755:289822 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011170:39195:39257 [0] NCCL INFO Connected all trees
nid011170:39195:39257 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011170:39195:39257 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011170:39197:39258 [2] NCCL INFO Connected all trees
nid011170:39196:39256 [1] NCCL INFO Connected all trees
nid011170:39196:39256 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011170:39197:39258 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
nid011170:39196:39256 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011170:39197:39258 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 2 p2p channels per peer
nid011169:289754:289820 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011169:289754:289820 [0] NCCL INFO ncclCommInitRank comm 0xaaaed3672f40 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011169:289755:289822 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011169:289756:289821 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011169:289755:289822 [1] NCCL INFO ncclCommInitRank comm 0xaaad7c1e20a0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011169:289757:289823 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011169:289756:289821 [2] NCCL INFO ncclCommInitRank comm 0xaaacb7991bc0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011169:289757:289823 [3] NCCL INFO ncclCommInitRank comm 0xaaad693e1c60 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011170:39197:39258 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011170:39198:39255 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011170:39197:39258 [2] NCCL INFO ncclCommInitRank comm 0xaaae160b37f0 rank 6 nranks 8 cudaDev 2 nvmlDev 2 busId 2901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011170:39198:39255 [3] NCCL INFO ncclCommInitRank comm 0xaaacc4801a80 rank 7 nranks 8 cudaDev 3 nvmlDev 3 busId 3901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011170:39196:39256 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011170:39195:39257 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v2, using internal tuner instead.
nid011170:39196:39256 [1] NCCL INFO ncclCommInitRank comm 0xaaae391232b0 rank 5 nranks 8 cudaDev 1 nvmlDev 1 busId 1901000 commId 0x3c93adf3209e108c - Init COMPLETE
nid011170:39195:39257 [0] NCCL INFO ncclCommInitRank comm 0xaaab58861270 rank 4 nranks 8 cudaDev 0 nvmlDev 0 busId 901000 commId 0x3c93adf3209e108c - Init COMPLETE
[Memory] phase=after_init ranks=8 max_gpu_usage=2.65% max_cpu_usage=16.97% bottleneck=unknown
2026-01-07 01:01:20.348 | INFO     | utils.memory_utils:distributed_memory_snapshot:268 - [Memory] phase=after_init ranks=8 max_gpu_usage=2.65% max_cpu_usage=16.97% bottleneck=unknown
node=nid011169 rank=0 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.348 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=0 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=0 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=0 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=1 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
node=nid011169 rank=1 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
node=nid011169 rank=1 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
node=nid011169 rank=1 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.95 GB
node=nid011169 rank=2 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=2 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=2 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=2 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=3 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=3 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=3 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=3 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=4 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=4 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=4 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=4 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=5 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=5 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.349 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=5 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=5 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=6 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=7 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=7 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=7 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011170 rank=7 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.350 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
æˆ‘æ˜¯è¿›ç¨‹ 3ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 3æˆ‘æ˜¯è¿›ç¨‹ 2ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 2æˆ‘æ˜¯è¿›ç¨‹ 1ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 1


æˆ‘æ˜¯è¿›ç¨‹ 0ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 0
æˆ‘æ˜¯è¿›ç¨‹ 5ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 1æˆ‘æ˜¯è¿›ç¨‹ 4ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 0æˆ‘æ˜¯è¿›ç¨‹ 7ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 3æˆ‘æ˜¯è¿›ç¨‹ 6ï¼Œæ€»è¿›ç¨‹æ•° 8ï¼Œå½“å‰è®¾å¤‡: 2



2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 0
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 0
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 2
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 3
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 1
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 2
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 1
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 3
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 0
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 3
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 1
2026-01-07 01:01:20,350 - evaluation - INFO - the rank is 2
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 1
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 0
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 3
2026-01-07:01:01:20,350 INFO     [optimize.py:58] the rank is 2
[Memory] phase=before_LM_build ranks=8 max_gpu_usage=2.65% max_cpu_usage=16.98% bottleneck=unknown
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:268 - [Memory] phase=before_LM_build ranks=8 max_gpu_usage=2.65% max_cpu_usage=16.98% bottleneck=unknown
node=nid011169 rank=0 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=0 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=0 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=0 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011169 rank=1 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011169 rank=1 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011169 rank=1 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011169 rank=1 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=0.98 GB
node=nid011169 rank=2 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=2 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=2 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=2 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=3 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.361 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=3 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=3 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011169 rank=3 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011170 rank=4 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011170 rank=4 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011170 rank=4 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011170 rank=4 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.05 GB
node=nid011170 rank=5 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=5 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=5 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=5 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=6 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=7 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=0 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=7 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=1 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=7 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=2 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
node=nid011170 rank=7 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
2026-01-07 01:01:20.362 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=3 nvml used/total=2.53 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.02 GB
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:03,  7.38it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:03,  8.22it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:00<00:03,  8.63it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:00<00:03,  8.30it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:16,  1.72it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:17,  1.70it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:17,  1.68it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:00<00:03,  8.17it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:08,  3.17it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:09,  3.08iLoading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:03,  7.44it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:03,  8.26it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:00<00:03,  8.71it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:00<00:03,  6.96it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:20,  1.43it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:19,  1.49it/s]Loading checkpoint shards:   3%|â–Ž         | 1/30 [00:00<00:21,  1.38it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:00<00:05,  4.52it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:12,  2.21it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:12,  2.28it/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:12,  2.20it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:04,  5.22it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:08,  3.31it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:07,  3.40it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:08,  3.31it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:03,  5.83it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:05,  4.34it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:05,  4.44it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:05,  4.35it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:01<00:03,  6.35it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:04,  5.24it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:04,  5.34it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:04,  5.27it/s]Loading ct/s]Loading checkpoint shards:   7%|â–‹         | 2/30 [00:00<00:09,  2.95it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:00<00:03,  6.52it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:09,  2.73it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:10,  2.66it/s]Loading checkpoint shards:  10%|â–ˆ         | 3/30 [00:01<00:10,  2.62it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:05,  3.86it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:10,  2.54it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:10,  2.55it/s]Loading checkpoint shards:  13%|â–ˆâ–Ž        | 4/30 [00:01<00:10,  2.45it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:01<00:06,  3.44it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:07,  3.24it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:07,  3.22it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 5/30 [00:01<00:07,  3.27it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:01<00:03,  6.78it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:04,  5.98it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:03,  6.09it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:03,  6.03it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:03,  6.57it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:03,  6.66it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:01<00:02,  7.10it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:03,  6.62it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:01<00:03,  7.01it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:01<00:03,  7.10it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:01<00:03,  7.07it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:01<00:02,  7.34it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:01<00:02,  7.4heckpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:01<00:05,  4.18it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:05,  4.06it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:05,  4.05it/s]Loading checkpoint shards:  20%|â–ˆâ–ˆ        | 6/30 [00:01<00:05,  4.10it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:01<00:04,  4.86it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:04,  4.83it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:01<00:04,  4.82it/s]Loading checkpoint shards:  23%|â–ˆâ–ˆâ–Ž       | 7/30 [00:02<00:04,  4.89it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:03,  5.47it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:02<00:03,  5.52it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:02<00:03,  5.53it/s]Loading checkpoint shards:  27%|â–ˆâ–ˆâ–‹       | 8/30 [00:02<00:03,  5.60it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:03,  2it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:01<00:02,  7.34it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:01<00:02,  7.40it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:01<00:02,  7.54it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:01<00:02,  7.65it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:01<00:02,  7.59it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:01<00:02,  7.64it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:01<00:02,  7.68it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:02,  7.81it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:02,  7.76it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:02,  7.81it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:02,  7.80it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:02,  7.90it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:02,  7.87it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:02,  7.91it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  7.85it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  7.97it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  7.99it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  7.94it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:02<00:01,  7.88it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:01,  8.05it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:01,  8.01it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:01,  8.00it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:02<00:01,  7.93it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  8.07it/s]Loading checkpoint shards: 5.98it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:02<00:03,  6.13it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:02<00:03,  6.11it/s]Loading checkpoint shards:  30%|â–ˆâ–ˆâ–ˆ       | 9/30 [00:02<00:03,  6.19it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  6.39it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:02<00:03,  6.60it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:02<00:03,  6.60it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:02<00:02,  6.67it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:02,  6.72it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:02,  6.98it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:02,  6.97it/s]Loading checkpoint shards:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [00:02<00:02,  7.04it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:02,  6.94it/s]Loading checkpoint sh 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  8.04it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  8.04it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:02<00:01,  7.95it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:02<00:01,  8.07it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:02<00:01,  8.07it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:02<00:01,  8.06it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:02<00:01,  7.99it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:02<00:01,  8.08it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:02<00:01,  8.08it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:02<00:01,  8.07it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:02<00:01,  7.97it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:02<00:01,  8.08it/s]Loadingards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:02,  7.28it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:02,  7.27it/s]Loading checkpoint shards:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:02<00:02,  7.34it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:02<00:01,  7.12it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  7.52it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  7.52it/s]Loading checkpoint shards:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [00:02<00:02,  7.58it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:02<00:01,  7.27it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:02,  7.70it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:02,  7.70it/s]Loading checkpoint shards:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [00:02<00:02,  7.76it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:02<00:01,  7.38it/s]Loading checkpoint shards checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:02<00:01,  8.07it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:02<00:01,  8.06it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:02<00:01,  7.97it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  8.07it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  8.07it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  8.05it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:01,  7.97it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  8.06it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  8.04it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  8.02it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  7.51it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  7.84it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  7.84it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [00:02<00:01,  7.90it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  7.51it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:03<00:01,  7.96it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:03<00:01,  7.94it/s]Loading checkpoint shards:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:03<00:01,  8.02it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:03<00:01,  8.04it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:03<00:01,  8.02it/s]Loading checkpoint shards:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [00:03<00:01,  8.08it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  7.26it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:03<00:01,  8.05it/s]Loadingˆ   | 21/30 [00:03<00:01,  8.04it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:03<00:01,  8.03it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:03<00:01,  8.03it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:03<00:00,  7.57it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:00,  8.04it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:00,  8.03it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:00,  8.02it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:03<00:00,  7.55it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  8.03it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  8.05it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  8.00it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:03<00:0 checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:03<00:01,  8.05it/s]Loading checkpoint shards:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [00:03<00:01,  8.12it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:03<00:01,  7.41it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  8.06it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  8.07it/s]Loading checkpoint shards:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [00:03<00:01,  8.12it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:01,  7.54it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  8.07it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  8.07it/s]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:03<00:01,  8.12it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  7.64it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–0,  7.55it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:03<00:00,  8.08it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:03<00:00,  8.03it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:03<00:00,  8.01it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:03<00:00,  7.56it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:03<00:00,  8.06it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:03<00:00,  8.04it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:03<00:00,  8.02it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:03<00:00,  7.55it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:03<00:00,  8.07it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:03<00:00,  8.07it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:03<00:00,  7ˆ   | 21/30 [00:03<00:01,  8.08it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:03<00:01,  8.05it/s]Loading checkpoint shards:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [00:03<00:01,  8.11it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:03<00:00,  7.72it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:00,  8.06it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:00,  8.06it/s]Loading checkpoint shards:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [00:03<00:00,  8.10it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:03<00:00,  7.67it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  8.06it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  8.06it/s]Loading checkpoint shards:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [00:03<00:00,  8.08it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:04<00:0.99it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:03<00:00,  7.60it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  8.09it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  8.08it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  8.02it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  7.82it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  7.31it/s]
0,  7.66it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:04<00:00,  8.06it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:04<00:00,  8.06it/s]Loading checkpoint shards:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:04<00:00,  8.05it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  7.64it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:04<00:00,  8.05it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:04<00:00,  8.05it/s]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [00:04<00:00,  8.07it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  7.62it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:04<00:00,  8.06it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:04<00:00,  8.04it/s]Loading checkpoint shards:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [00:04<00:00,  8Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  8.13it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  8.13it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  8.09it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  8.25it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  8.25it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  8.23it/s]The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  8.37it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.81it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  8.37it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.86it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  8.34it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.79it/s]
.07it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  7.61it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  8.07it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  8.06it/s]Loading checkpoint shards:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [00:04<00:00,  8.08it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  7.79it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.62it/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  8.16it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  8.14it/s]Loading checkpoint shards:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [00:04<00:00,  8.17it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  8.29it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  8.29it/s]Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [00:04<00:00,  8.29it/s]The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  8.37it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  8.37it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.23it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.23it/s]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  8.38it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:04<00:00,  6.22it/s]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. 
The class this function is called from is 'LlamaTokenizerFast'.
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
FSDPLlamaModel(
  (embed_tokens): QuantEmbedding(
    128256, 8192
    (act_quantizer): Quantizer()
  )
  (layers): ModuleList(
    (0-79): 80 x FSDPQuantDecoderLayer(
      (self_attn): QuantAttention(
        (q_proj): QuantLinear(
          in_features=8192, out_features=8192, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (k_proj): QuantLinear(
          in_features=8192, out_features=1024, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (ropeq): QuantROPE(
          (rope_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (ropek): QuantROPE(
          (rope_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (v_proj): QuantLinear(
          in_features=8192, out_features=1024, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (o_proj): QuantLinear(
          in_features=8192, out_features=8192, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (qk_matmul): QuantMatmul(
          (act_quantizer): Quantizer()
        )
        (pv_matmul): QuantMatmul(
          (act_quantizer): Quantizer()
        )
        (softmax): QuantSoftmax(
          (act_quantizer): Quantizer()
        )
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (mlp): QuantMLP(
        (gate_proj): QuantLinear(
          in_features=8192, out_features=28672, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (down_proj): QuantLinear(
          in_features=28672, out_features=8192, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (up_proj): QuantLinear(
          in_features=8192, out_features=28672, bias=False
          (weight_quantizer): Quantizer()
          (act_quantizer): Quantizer()
        )
        (silu): QuantSiLU(
          (act_quantizer): Quantizer()
        )
        (mul): QuantMul(
          (act_quantizer): Quantizer()
        )
      )
      (input_layernorm): QuantRMSNorm(
        (act_quantizer): Quantizer()
      )
      (post_attention_layernorm): QuantRMSNorm(
        (act_quantizer): Quantizer()
      )
      (resadd1): QuantAdd(
        (act_quantizer): Quantizer()
      )
      (resadd2): QuantAdd(
        (act_quantizer): Quantizer()
      )
      (R_S_modules): ModuleDict()
    )
  )
  (norm): QuantRMSNorm(
    (act_quantizer): Quantizer()
  )
)
[Memory] phase=after_LM_build ranks=8 max_gpu_usage=25.09% max_cpu_usage=30.48% bottleneck=unknown
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:268 - [Memory] phase=after_LM_build ranks=8 max_gpu_usage=25.09% max_cpu_usage=30.48% bottleneck=unknown
node=nid011169 rank=0 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=1.24 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=1.24 GB
node=nid011169 rank=0 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
node=nid011169 rank=0 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
node=nid011169 rank=0 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
node=nid011169 rank=1 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.15 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=2 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=2 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=2 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.22 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.22 GB
node=nid011169 rank=2 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.151 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.22 GB
node=nid011170 rank=4 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=1.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=5 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.16 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=6 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.47 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.47 GB
node=nid011170 rank=6 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.47 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.47 GB
node=nid011170 rank=6 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.47 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.47 GB
node=nid011170 rank=6 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.47 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.47 GB
node=nid011170 rank=7 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.05 GB
2026-01-07 01:01:57.152 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.10 GB proc_rss=3.05 GB
[Memory] phase=before_generate_rotate_parameters ranks=8 max_gpu_usage=25.09% max_cpu_usage=30.48% bottleneck=unknown
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:268 - [Memory] phase=before_generate_rotate_parameters ranks=8 max_gpu_usage=25.09% max_cpu_usage=30.48% bottleneck=unknown
node=nid011169 rank=0 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=1.24 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=1.24 GB
node=nid011169 rank=0 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
node=nid011169 rank=0 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
node=nid011169 rank=0 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.24 GB
node=nid011169 rank=1 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.15 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=2 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
node=nid011169 rank=2 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
node=nid011169 rank=2 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.26 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.26 GB
node=nid011169 rank=2 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
node=nid011169 rank=3 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.22 GB
node=nid011170 rank=4 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=1.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=5 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.16 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:01:57.443 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=6 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
node=nid011170 rank=6 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
node=nid011170 rank=6 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.51 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.51 GB
node=nid011170 rank=6 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
node=nid011170 rank=7 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=0 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=1 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=2 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.05 GB
2026-01-07 01:01:57.444 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=3 nvml used/total=23.98 GB/95.58 GB torch alloc/resv/max=18.14 GB/21.45 GB/20.34 GB proc_rss=3.05 GB
[Memory] phase=rotate_layer_0_start ranks=8 max_gpu_usage=22.61% max_cpu_usage=29.14% bottleneck=unknown
Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]2026-01-07 01:02:00.261 | INFO     | utils.memory_utils:distributed_memory_snapshot:268 - [Memory] phase=rotate_layer_0_start ranks=8 max_gpu_usage=22.61% max_cpu_usage=29.14% bottleneck=unknown
node=nid011169 rank=0 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=18.89 GB/19.08 GB/20.34 GB proc_rss=1.27 GB
2026-01-07 01:02:00.261 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=18.89 GB/19.08 GB/20.34 GB proc_rss=1.27 GB
node=nid011169 rank=0 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.27 GB
2026-01-07 01:02:00.261 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.27 GB
node=nid011169 rank=0 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.27 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.27 GB
node=nid011169 rank=0 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.27 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=0 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.27 GB
node=nid011169 rank=1 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.15 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=1 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=1 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.15 GB
node=nid011169 rank=2 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
node=nid011169 rank=2 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
node=nid011169 rank=2 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.26 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.26 GB
node=nid011169 rank=2 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=2 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.26 GB
node=nid011169 rank=3 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=0 nvml used/total=21.61 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.22 GB
node=nid011169 rank=3 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011169 rank=3 gpu=3 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.22 GB
node=nid011170 rank=4 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=1.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=4 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=4 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=1.22 GB
node=nid011170 rank=5 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.16 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=5 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=5 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.16 GB
node=nid011170 rank=6 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
node=nid011170 rank=6 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
2026-01-07 01:02:00.262 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
node=nid011170 rank=6 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.51 GB
2026-01-07 01:02:00.263 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.51 GB
node=nid011170 rank=6 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
2026-01-07 01:02:00.263 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=6 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.51 GB
node=nid011170 rank=7 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:02:00.263 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=0 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:02:00.263 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=1 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
2026-01-07 01:02:00.263 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=2 nvml used/total=21.12 GB/95.58 GB torch alloc/resv/max=0.00 GB/0.00 GB/0.00 GB proc_rss=3.05 GB
node=nid011170 rank=7 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.05 GB
2026-01-07 01:02:00.263 | INFO     | utils.memory_utils:distributed_memory_snapshot:277 - node=nid011170 rank=7 gpu=3 nvml used/total=21.11 GB/95.58 GB torch alloc/resv/max=18.39 GB/18.58 GB/20.34 GB proc_rss=3.05 GB
Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]



Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:00<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]Generate Rotating Parameters:   0%|          | 0/80 [00:03<?, ?layer/s]



[rank3]: Traceback (most recent call last):
[rank3]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank3]:     train()
[rank3]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank3]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank3]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank3]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank3]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank3]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank3]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank3]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank3]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank3]:     return disable_fn(*args, **kwargs)
[rank3]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank3]:     return fn(*args, **kwargs)
[rank3]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank3]:     return DTensor._op_dispatcher.dispatch(
[rank3]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank3]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank3]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank3]:     return self._op(*args, **kwargs)
[rank3]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank3]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank3]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank3]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank2]: Traceback (most recent call last):
[rank2]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank2]:     train()
[rank2]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank2]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank2]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank2]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank2]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank2]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank2]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank2]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank2]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank2]:     return disable_fn(*args, **kwargs)
[rank2]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank2]:     return fn(*args, **kwargs)
[rank2]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank2]:     return DTensor._op_dispatcher.dispatch(
[rank2]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank2]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank2]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank2]:     return self._op(*args, **kwargs)
[rank2]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank2]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank2]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank2]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank1]: Traceback (most recent call last):
[rank1]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank1]:     train()
[rank1]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank1]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank1]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank1]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank1]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank1]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank1]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank1]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank1]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank1]:     return disable_fn(*args, **kwargs)
[rank1]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank1]:     return DTensor._op_dispatcher.dispatch(
[rank1]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank1]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank1]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank1]:     return self._op(*args, **kwargs)
[rank1]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank1]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank1]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank1]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank0]: Traceback (most recent call last):
[rank0]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank0]:     train()
[rank0]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank0]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank0]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank0]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank0]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank0]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank0]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank0]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank0]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank0]:     return disable_fn(*args, **kwargs)
[rank0]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank0]:     return DTensor._op_dispatcher.dispatch(
[rank0]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank0]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank0]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank0]:     return self._op(*args, **kwargs)
[rank0]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank0]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank0]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank0]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank6]: Traceback (most recent call last):
[rank6]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank6]:     train()
[rank6]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank6]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank6]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank6]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank6]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank6]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank6]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank6]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank6]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank6]:     return disable_fn(*args, **kwargs)
[rank6]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank6]:     return fn(*args, **kwargs)
[rank6]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank6]:     return DTensor._op_dispatcher.dispatch(
[rank6]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank6]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank6]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank6]:     return self._op(*args, **kwargs)
[rank6]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank6]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank6]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank6]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank5]: Traceback (most recent call last):
[rank5]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank5]:     train()
[rank5]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank5]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank5]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank5]:     return func(*args, **kwargs)
[rank5]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank5]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank5]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank5]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank5]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank5]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank5]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank5]:     return disable_fn(*args, **kwargs)
[rank5]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank5]:     return fn(*args, **kwargs)
[rank5]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank5]:     return DTensor._op_dispatcher.dispatch(
[rank5]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank5]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank5]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank5]:     return self._op(*args, **kwargs)
[rank5]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank5]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank5]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank5]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank7]: Traceback (most recent call last):
[rank7]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank7]:     train()
[rank7]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank7]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank7]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank7]:     return func(*args, **kwargs)
[rank7]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank7]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank7]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank7]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank7]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank7]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank7]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank7]:     return disable_fn(*args, **kwargs)
[rank7]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank7]:     return fn(*args, **kwargs)
[rank7]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank7]:     return DTensor._op_dispatcher.dispatch(
[rank7]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank7]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank7]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank7]:     return self._op(*args, **kwargs)
[rank7]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank7]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank7]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank7]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[rank4]: Traceback (most recent call last):
[rank4]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 285, in <module>
[rank4]:     train()
[rank4]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/optimize.py", line 86, in train
[rank4]:     lm.generate_rotate_parameters(args=training_args,ptq_args=ptq_args)
[rank4]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank4]:     return func(*args, **kwargs)
[rank4]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/train_utils/model_utils.py", line 225, in generate_rotate_parameters
[rank4]:     hadamard_utils.apply_exact_had_to_linear(layer.mlp.down_proj,had_dim=-1,output=False)
[rank4]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 163, in apply_exact_had_to_linear
[rank4]:     W_ = matmul_hadU_cuda(W_, had_K, K)
[rank4]:   File "/lus/lfs1aip2/projects/u5hv/lionky.u5hv/work_home/quantization-evaluation/utils/hadamard_utils.py", line 135, in matmul_hadU_cuda
[rank4]:     input = HadamardTransform.apply(input.contiguous()) / torch.tensor(n).sqrt()
[rank4]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
[rank4]:     return disable_fn(*args, **kwargs)
[rank4]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
[rank4]:     return fn(*args, **kwargs)
[rank4]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_api.py", line 346, in __torch_dispatch__
[rank4]:     return DTensor._op_dispatcher.dispatch(
[rank4]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/tensor/_dispatch.py", line 216, in dispatch
[rank4]:     local_results = op_call(*local_tensor_args, **op_info.local_kwargs)
[rank4]:   File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/_ops.py", line 723, in __call__
[rank4]:     return self._op(*args, **kwargs)
[rank4]: RuntimeError: CUDA error: an illegal memory access was encountered
[rank4]: CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
[rank4]: For debugging consider passing CUDA_LAUNCH_BLOCKING=1
[rank4]: Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.


nid011169:289757:289887 [3] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289757:289887 [3] NCCL INFO init.cc:1952 -> 1

nid011169:289757:289887 [3] init.cc:2083 NCCL WARN commReclaim: comm 0xaaad693e1c60 (rank = 3) in abort, error 1
nid011169:289757:289834 [3] NCCL INFO [Service thread] Connection closed by localRank 3

nid011169:289755:289888 [1] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289755:289888 [1] NCCL INFO init.cc:1952 -> 1

nid011169:289755:289888 [1] init.cc:2083 NCCL WARN commReclaim: comm 0xaaad7c1e20a0 (rank = 1) in abort, error 1
nid011169:289755:289832 [1] NCCL INFO [Service thread] Connection closed by localRank 1

nid011169:289756:289889 [2] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289756:289889 [2] NCCL INFO init.cc:1952 -> 1

nid011169:289756:289889 [2] init.cc:2083 NCCL WARN commReclaim: comm 0xaaacb7991bc0 (rank = 2) in abort, error 1
nid011169:289756:289830 [2] NCCL INFO [Service thread] Connection closed by localRank 2
[rank0]:[W107 01:02:01.920614496 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

nid011169:289754:289890 [0] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289754:289890 [0] NCCL INFO init.cc:1952 -> 1

nid011169:289754:289890 [0] init.cc:2083 NCCL WARN commReclaim: comm 0xaaaed3672f40 (rank = 0) in abort, error 1
nid011169:289754:289828 [0] NCCL INFO [Service thread] Connection closed by localRank 0

nid011170:39196:39301 [1] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'

nid011170:39198:39303 [3] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'

nid011170:39197:39302 [2] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011170:39196:39301 [1] NCCL INFO init.cc:1952 -> 1
nid011170:39197:39302 [2] NCCL INFO init.cc:1952 -> 1
nid011170:39198:39303 [3] NCCL INFO init.cc:1952 -> 1

nid011170:39196:39301 [1] init.cc:2083 NCCL WARN commReclaim: comm 0xaaae391232b0 (rank = 5) in abort, error 1

nid011170:39197:39302 [2] init.cc:2083 NCCL WARN commReclaim: comm 0xaaae160b37f0 (rank = 6) in abort, error 1

nid011170:39198:39303 [3] init.cc:2083 NCCL WARN commReclaim: comm 0xaaacc4801a80 (rank = 7) in abort, error 1
nid011170:39196:39269 [1] NCCL INFO [Service thread] Connection closed by localRank 1
nid011170:39197:39265 [2] NCCL INFO [Service thread] Connection closed by localRank 2
nid011170:39198:39267 [3] NCCL INFO [Service thread] Connection closed by localRank 3
[rank4]:[W107 01:02:01.172544345 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

nid011170:39195:39304 [0] misc/strongstream.cc:395 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011170:39195:39304 [0] NCCL INFO init.cc:1952 -> 1

nid011170:39195:39304 [0] init.cc:2083 NCCL WARN commReclaim: comm 0xaaab58861270 (rank = 4) in abort, error 1
nid011170:39195:39263 [0] NCCL INFO [Service thread] Connection closed by localRank 0

nid011169:289757:289834 [3] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289757:289834 [3] NCCL INFO include/alloc.h:246 -> 1

nid011169:289757:289834 [3] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289757:289834 [3] NCCL INFO include/alloc.h:246 -> 1

nid011169:289757:289834 [3] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289757:289834 [3] NCCL INFO transport/net.cc:956 -> 1
nid011169:289757:289834 [3] NCCL INFO proxy.cc:984 -> 1
nid011169:289757:289834 [3] NCCL INFO proxy.cc:1000 -> 1

nid011169:289757:289887 [3] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289757:289887 [3] NCCL INFO include/alloc.h:246 -> 1
nid011169:289757:289887 [3] NCCL INFO transport/p2p.cc:557 -> 1
nid011169:289757:289887 [3] NCCL INFO channel.cc:159 -> 1
nid011169:289757:289887 [3] NCCL INFO init.cc:210 -> 1
nid011169:289757:289887 [3] NCCL INFO init.cc:1986 -> 1

nid011169:289757:289887 [3] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaad693e1c60 rank 3 failed in destroy/abort, error 1
nid011169:289757:289887 [3] NCCL INFO comm 0xaaad693e1c60 rank 3 nranks 8 cudaDev 3 busId 3901000 - Abort COMPLETE

nid011169:289755:289832 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289755:289832 [1] NCCL INFO include/alloc.h:246 -> 1

nid011169:289755:289832 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289755:289832 [1] NCCL INFO include/alloc.h:246 -> 1

nid011169:289755:289832 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289755:289832 [1] NCCL INFO include/alloc.h:246 -> 1

nid011169:289755:289832 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289755:289832 [1] NCCL INFO include/alloc.h:246 -> 1

nid011169:289755:289832 [1] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289755:289832 [1] NCCL INFO transport/net.cc:1000 -> 1
nid011169:289755:289832 [1] NCCL INFO proxy.cc:988 -> 1
nid011169:289755:289832 [1] NCCL INFO proxy.cc:1000 -> 1

nid011169:289755:289888 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289755:289888 [1] NCCL INFO include/alloc.h:246 -> 1
nid011169:289755:289888 [1] NCCL INFO transport/p2p.cc:541 -> 1
nid011169:289755:289888 [1] NCCL INFO channel.cc:158 -> 1
nid011169:289755:289888 [1] NCCL INFO init.cc:210 -> 1
nid011169:289755:289888 [1] NCCL INFO init.cc:1986 -> 1

nid011169:289755:289888 [1] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaad7c1e20a0 rank 1 failed in destroy/abort, error 1
nid011169:289755:289888 [1] NCCL INFO comm 0xaaad7c1e20a0 rank 1 nranks 8 cudaDev 1 busId 1901000 - Abort COMPLETE

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO include/alloc.h:246 -> 1

nid011169:289756:289830 [2] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289756:289830 [2] NCCL INFO transport/net.cc:956 -> 1
nid011169:289756:289830 [2] NCCL INFO proxy.cc:984 -> 1
nid011169:289756:289830 [2] NCCL INFO proxy.cc:1000 -> 1

nid011169:289756:289889 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289756:289889 [2] NCCL INFO include/alloc.h:246 -> 1
nid011169:289756:289889 [2] NCCL INFO transport/p2p.cc:541 -> 1
nid011169:289756:289889 [2] NCCL INFO channel.cc:158 -> 1
nid011169:289756:289889 [2] NCCL INFO init.cc:210 -> 1
nid011169:289756:289889 [2] NCCL INFO init.cc:1986 -> 1

nid011169:289756:289889 [2] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaacb7991bc0 rank 2 failed in destroy/abort, error 1
nid011169:289756:289889 [2] NCCL INFO comm 0xaaacb7991bc0 rank 2 nranks 8 cudaDev 2 busId 2901000 - Abort COMPLETE

nid011169:289754:289828 [0] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011169:289754:289828 [0] NCCL INFO transport/net.cc:1000 -> 1
nid011169:289754:289828 [0] NCCL INFO proxy.cc:988 -> 1
nid011169:289754:289828 [0] NCCL INFO proxy.cc:1000 -> 1

nid011169:289754:289890 [0] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011169:289754:289890 [0] NCCL INFO include/alloc.h:246 -> 1
nid011169:289754:289890 [0] NCCL INFO transport/p2p.cc:541 -> 1
nid011169:289754:289890 [0] NCCL INFO channel.cc:158 -> 1
nid011169:289754:289890 [0] NCCL INFO init.cc:210 -> 1
nid011169:289754:289890 [0] NCCL INFO init.cc:1986 -> 1

nid011169:289754:289890 [0] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaaed3672f40 rank 0 failed in destroy/abort, error 1
nid011169:289754:289890 [0] NCCL INFO comm 0xaaaed3672f40 rank 0 nranks 8 cudaDev 0 busId 901000 - Abort COMPLETE

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'

nid011170:39198:39267 [3] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1
nid011170:39198:39267 [3] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39198:39267 [3] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39198:39267 [3] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39198:39267 [3] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39198:39267 [3] NCCL INFO transport/net.cc:956 -> 1
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1
nid011170:39198:39267 [3] NCCL INFO proxy.cc:984 -> 1
nid011170:39198:39267 [3] NCCL INFO proxy.cc:1000 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO include/alloc.h:246 -> 1

nid011170:39197:39265 [2] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011170:39197:39265 [2] NCCL INFO transport/net.cc:956 -> 1
nid011170:39197:39265 [2] NCCL INFO proxy.cc:984 -> 1
nid011170:39197:39265 [2] NCCL INFO proxy.cc:1000 -> 1

nid011170:39196:39269 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39196:39269 [1] NCCL INFO include/alloc.h:246 -> 1

nid011170:39196:39269 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39196:39269 [1] NCCL INFO include/alloc.h:246 -> 1

nid011170:39196:39269 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39196:39269 [1] NCCL INFO include/alloc.h:246 -> 1

nid011170:39196:39269 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39196:39269 [1] NCCL INFO include/alloc.h:246 -> 1

nid011170:39196:39269 [1] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011170:39196:39269 [1] NCCL INFO transport/net.cc:1000 -> 1
nid011170:39196:39269 [1] NCCL INFO proxy.cc:988 -> 1
nid011170:39196:39269 [1] NCCL INFO proxy.cc:1000 -> 1

nid011170:39197:39302 [2] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39197:39302 [2] NCCL INFO include/alloc.h:246 -> 1
nid011170:39197:39302 [2] NCCL INFO transport/p2p.cc:541 -> 1
nid011170:39197:39302 [2] NCCL INFO channel.cc:158 -> 1
nid011170:39197:39302 [2] NCCL INFO init.cc:210 -> 1
nid011170:39197:39302 [2] NCCL INFO init.cc:1986 -> 1

nid011170:39198:39303 [3] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'

nid011170:39197:39302 [2] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaae160b37f0 rank 6 failed in destroy/abort, error 1
nid011170:39198:39303 [3] NCCL INFO include/alloc.h:246 -> 1
nid011170:39197:39302 [2] NCCL INFO comm 0xaaae160b37f0 rank 6 nranks 8 cudaDev 2 busId 2901000 - Abort COMPLETE
nid011170:39198:39303 [3] NCCL INFO transport/p2p.cc:557 -> 1
nid011170:39198:39303 [3] NCCL INFO channel.cc:159 -> 1
nid011170:39198:39303 [3] NCCL INFO init.cc:210 -> 1
nid011170:39198:39303 [3] NCCL INFO init.cc:1986 -> 1

nid011170:39198:39303 [3] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaacc4801a80 rank 7 failed in destroy/abort, error 1
nid011170:39198:39303 [3] NCCL INFO comm 0xaaacc4801a80 rank 7 nranks 8 cudaDev 3 busId 3901000 - Abort COMPLETE

nid011170:39196:39301 [1] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39196:39301 [1] NCCL INFO include/alloc.h:246 -> 1
nid011170:39196:39301 [1] NCCL INFO transport/p2p.cc:541 -> 1
nid011170:39196:39301 [1] NCCL INFO channel.cc:158 -> 1
nid011170:39196:39301 [1] NCCL INFO init.cc:210 -> 1
nid011170:39196:39301 [1] NCCL INFO init.cc:1986 -> 1

nid011170:39196:39301 [1] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaae391232b0 rank 5 failed in destroy/abort, error 1
nid011170:39196:39301 [1] NCCL INFO comm 0xaaae391232b0 rank 5 nranks 8 cudaDev 1 busId 1901000 - Abort COMPLETE

nid011170:39195:39263 [0] include/alloc.h:39 NCCL WARN Cuda failure 'an illegal memory access was encountered'
nid011170:39195:39263 [0] NCCL INFO transport/net.cc:1000 -> 1
nid011170:39195:39263 [0] NCCL INFO proxy.cc:988 -> 1
nid011170:39195:39263 [0] NCCL INFO proxy.cc:1000 -> 1

nid011170:39195:39304 [0] include/alloc.h:125 NCCL WARN Cuda failure 700 'an illegal memory access was encountered'
nid011170:39195:39304 [0] NCCL INFO include/alloc.h:246 -> 1
nid011170:39195:39304 [0] NCCL INFO transport/p2p.cc:541 -> 1
nid011170:39195:39304 [0] NCCL INFO channel.cc:158 -> 1
nid011170:39195:39304 [0] NCCL INFO init.cc:210 -> 1
nid011170:39195:39304 [0] NCCL INFO init.cc:1986 -> 1

nid011170:39195:39304 [0] init.cc:2118 NCCL WARN commReclaim: cleanup comm 0xaaab58861270 rank 4 failed in destroy/abort, error 1
nid011170:39195:39304 [0] NCCL INFO comm 0xaaab58861270 rank 4 nranks 8 cudaDev 0 busId 901000 - Abort COMPLETE
W0107 01:02:02.157031 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 289755 closing signal SIGTERM
W0107 01:02:02.157692 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 289756 closing signal SIGTERM
W0107 01:02:02.158028 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 289757 closing signal SIGTERM
W0107 01:02:02.444396 39178 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 39197 closing signal SIGTERM
E0107 01:02:02.535888 289742 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 289754) of binary: /projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/bin/python3.9
Traceback (most recent call last):
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
optimize.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-07_01:02:02
  host      : nid011169
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 289754)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
E0107 01:02:02.558619 39178 /lus/lfs1aip2/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 39195) of binary: /projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/bin/python3.9
Traceback (most recent call last):
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/projects/u5hv/lionky.u5hv/conda/conda_envs/quantization-eval/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
optimize.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2026-01-07_01:02:02
  host      : nid011170
  rank      : 5 (local_rank: 1)
  exitcode  : 1 (pid: 39196)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2026-01-07_01:02:02
  host      : nid011170
  rank      : 7 (local_rank: 3)
  exitcode  : 1 (pid: 39198)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-07_01:02:02
  host      : nid011170
  rank      : 4 (local_rank: 0)
  exitcode  : 1 (pid: 39195)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: nid011169: task 0: Exited with exit code 1
srun: Terminating StepId=1844190.0
srun: error: nid011170: task 1: Terminated
srun: Force Terminated StepId=1844190.0
